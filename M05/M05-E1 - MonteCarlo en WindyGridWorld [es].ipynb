{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkkr1HUBPOcL"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo: Método de MonteCarlo en el entorno WindyGridWorld\n",
    "\n",
    "En este ejemplo implementaremos el método de MonteCarlo de aprendizaje por refuerzo para buscar una solución óptima en el problema de WindyGridWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyFUy1qlDGku"
   },
   "source": [
    "## 1. El entorno __WindyGridWorld__\n",
    "\n",
    "El entorno __WindyGridWorld__ consiste en un agente que se mueve en una cuadrícula 7x10 (alto x ancho). En cada paso, el agente tiene 4 opciones de acción o movimiento: ARRIBA, ABAJO, DERECHA, IZQUIERDA. El agente siempre sale de la misma casilla [3, 0] y el juego termina cuando el agente llega a la casilla de llegada [3, 7]. \n",
    "\n",
    "El entorno se corresponde con el ejemplo 'Cuadrícula con viento' explicado en la sección 3.1.2. el módulo \"Métodos de Diferencia Temporal\". El problema radica en que hay un viento que empuja al agente hacia arriba en la parte central de la cuadrícula. Esto provoca que, aunque se ejecute una acción estándar, en la región central los estados resultantes se desplazan hacia arriba por un viento cuya fuerza varía entre columnas.\n",
    "\n",
    "<img src=\"../figs/GridWorld.png\">\n",
    "\n",
    "El código para implementar este entorno, que se encuentra disponible en el fichero adjunto `windy_gridworld_env.py`, ha sido adaptado del siguiente enlace:\n",
    "\n",
    "https://pypi.org/project/gym-gridworlds/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Métodos de Montecarlo\n",
    "\n",
    "Dado que el entorno es determinista, es factible encontrar una política óptima (que puede no ser única) que consiga el mayor retorno (y por tanto la trayectoria más corta).\n",
    "\n",
    "El objetivo de este apartado es realizar una estimación de la política óptima mediante los métodos de Montecarlo, en concreto estudiaremos el algoritmo *On-policy first-visit MC control (para políticas $\\epsilon$-soft)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, implementaremos el Algoritmo 3 explicado en el módulo \"Métodos de Montecarlo\" utilizando los siguientes parámetros:\n",
    "    \n",
    "- Número de episodios = 250000\n",
    "- Epsilon = 0.1\n",
    "- Factor de descuento = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solución adaptada de: https://github.com/dennybritz/reinforcement-learning/tree/master/MC\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import windy_gridworld_env as wge\n",
    "\n",
    "#Cargamos el entrono\n",
    "env = wge.WindyGridworldEnv()\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, num_Actions):\n",
    "    \"\"\"\n",
    "    Crea una política epsilon-greedy basada en una función de valor de acción Q y epsilon\n",
    "    \n",
    "    Args:\n",
    "        Q: Un diccionario cuya correspondencia es state -> action-values.\n",
    "           Cada valor es un array de numpy de longitud num_Actions (see below)\n",
    "        epsilon: La probabilidad de seleccionar una acción aleatoria (float entre 0 and 1).\n",
    "        num_Actions: Número de acciones del entorno. (en el caso del WIndyGridWorld es 4)\n",
    "    \n",
    "    Returns:\n",
    "        Una función que tome como argumento la observación y devuelva como resultado\n",
    "        las probabilidades de cada acción como un array de numpy de longitud num_Actions.\n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "\n",
    "        A = np.ones(num_Actions, dtype=float) * epsilon / num_Actions\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    return policy_fn\n",
    "\n",
    "def mc_control_on_policy_epsilon_greedy(env, num_episodes, discount=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Control mediante métodos de Montecarlo usando políticas Epsilon-Greedy\n",
    "    Encuentra una política epsilon-greedy.\n",
    "    \n",
    "    Args:\n",
    "        env: entorno OpenAI gym.\n",
    "        num_episodes: Número de episodios de la muestra.\n",
    "        discount: factor de descuento.\n",
    "        epsilon: La probabilidad de seleccionar una acción aleatoria (float entre 0 and 1)\n",
    "    \n",
    "    Returns:\n",
    "        Una tupla (Q, policy).\n",
    "        Q: Un diccionario cuya correspondencia es state -> action-values.\n",
    "        policy: Una función que toma como argumento la observación y devuelve como resultado\n",
    "                las probabilidades de cada acción\n",
    "    \"\"\"\n",
    "    \n",
    "    # Almacenamos la suma y el número de retornos de cada estado para calcular\n",
    "    # el promedio. Podríamos usar un array para guardar todos los retornos\n",
    "    # (como en el libro) pero es ineficiente en términos de memoria.\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # La función de valor de acción Q.\n",
    "    # Un diccionario anidado cuya correspondencia es state -> (action -> action-value).\n",
    "    # Inicialmente la inicializamos a cero\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # La política que estamos siguiendo\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Imprimimos en qué episodio estamos, útil para debugar.\n",
    "        if i_episode % 10 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generamos un episodio y lo almacenamos\n",
    "        # Un episodio es un array de las tuplas (state, action, reward)\n",
    "        episode = []\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        for t in range(1000): # limitamos el número de time-steps de cada episodio a 1000\n",
    "            probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Encontramos todos los pares (estado, acción) que hemos visitado en este episodio\n",
    "        # Convertimos cada estado en una tupla para poder usarlo como clave del diccionario\n",
    "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
    "        for state, action in sa_in_episode:\n",
    "            sa_pair = (state, action)\n",
    "            # Encontramos la primera aparición del par (estado, acción) en el episodio\n",
    "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
    "                                       if x[0] == state and x[1] == action)\n",
    "            # Sumamos todas las recompensas desde la primera aparición\n",
    "            G = sum([x[2]*(discount**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
    "            # Calculamos el retorno promedio para este estado en todos los episodios muestreados\n",
    "            returns_sum[sa_pair] += G\n",
    "            returns_count[sa_pair] += 1.0\n",
    "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
    "        \n",
    "        # La política se mejora implícitamente al ir cambiando los valores de Q\n",
    "    \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250000/250000."
     ]
    }
   ],
   "source": [
    "Q, policy = mc_control_on_policy_epsilon_greedy(env, num_episodes=250000, discount=1, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguidamente, implementaremos una función que imprima por pantalla la política óptima encontrada para cada celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(policy, height, width):\n",
    "\n",
    "    switch_action = {\n",
    "        0: \"U\",\n",
    "        1: \"R\",\n",
    "        2: \"D\",\n",
    "        3: \"L\"\n",
    "    }\n",
    "    for i in range(height):\n",
    "        print(\"------------------------------------------------------------------------------------------\")\n",
    "        for j in range(width):\n",
    "            arr = np.array(policy((i,j)))\n",
    "            act = int(np.where(arr == np.amax(arr))[0])\n",
    "            a = switch_action[act]\n",
    "            print(\"  %s  |\" % a, end=\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "  L  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  R  |  D  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  D  |  L  |  U  |  L  |  R  |  R  |  L  |  U  |  L  |  D  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  R  |  R  |  R  |  R  |  U  |  R  |  R  |  R  |  R  |  D  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  U  |  L  |  R  |  U  |  D  |  R  |  D  |  U  |  R  |  D  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  L  |  U  |  R  |  L  |  R  |  D  |  U  |  D  |  L  |  L  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  R  |  D  |  R  |  R  |  L  |  U  |  U  |  D  |  L  |  L  |\n",
      "------------------------------------------------------------------------------------------\n",
      "  L  |  R  |  U  |  D  |  U  |  U  |  U  |  U  |  U  |  L  |\n"
     ]
    }
   ],
   "source": [
    "print_policy(policy,7,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos un episodio con la política óptima encontrada y mostramos la trayectoria del agente y el retorno obtenido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode_MC(policy, env):\n",
    "    obs, info = env.reset()\n",
    "    t, total_reward, done = 0, 0, False\n",
    "    print(\"Obs inicial: {} \".format(obs))\n",
    "\n",
    "    switch_action = {\n",
    "            0: \"U\",\n",
    "            1: \"R\",\n",
    "            2: \"D\",\n",
    "            3: \"L\",\n",
    "        }\n",
    "\n",
    "    for t in range(1000): # limitamos el número de time-steps de cada episodio a 1000\n",
    "        \n",
    "        # Elegimos la política óptima en cada caso (el máximo de la política Epsilon-Greedy)\n",
    "        arr = np.array(policy(obs))\n",
    "        action = arr.argmax()\n",
    "    \n",
    "        # Ejecutamos la acción y esperamos la respuesta del entorno\n",
    "        new_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        obs = new_obs\n",
    "        print(\"Action: {} -> Obs: {} and reward: {}\".format(switch_action[action], obs, reward))\n",
    "\n",
    "        if t==999:\n",
    "            print(\"Number of time-septs exceeds 1000. STOP episode.\")\n",
    "            \n",
    "        total_reward += reward\n",
    "        t += 1\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    print(\"Episode finished after {} timesteps and reward was {} \".format(t, total_reward))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs inicial: (3, 0) \n",
      "Action: U -> Obs: (2, 0) and reward: -1\n",
      "Action: R -> Obs: (2, 1) and reward: -1\n",
      "Action: R -> Obs: (2, 2) and reward: -1\n",
      "Action: R -> Obs: (2, 3) and reward: -1\n",
      "Action: R -> Obs: (1, 4) and reward: -1\n",
      "Action: R -> Obs: (0, 5) and reward: -1\n",
      "Action: R -> Obs: (0, 6) and reward: -1\n",
      "Action: R -> Obs: (0, 7) and reward: -1\n",
      "Action: R -> Obs: (0, 8) and reward: -1\n",
      "Action: R -> Obs: (0, 9) and reward: -1\n",
      "Action: D -> Obs: (1, 9) and reward: -1\n",
      "Action: D -> Obs: (2, 9) and reward: -1\n",
      "Action: D -> Obs: (3, 9) and reward: -1\n",
      "Action: D -> Obs: (4, 9) and reward: -1\n",
      "Action: L -> Obs: (4, 8) and reward: -1\n",
      "Action: L -> Obs: (3, 7) and reward: -1\n",
      "Episode finished after 16 timesteps and reward was -16 \n"
     ]
    }
   ],
   "source": [
    "execute_episode_MC(policy,env)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "M2.883_PEC1_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
