{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario de Ciencia de Datos (<i>Data Science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Módulo 9: Ejemplo de DQN en CartPole\n",
    "\n",
    "\n",
    "En este _notebook_ vamos a ver un ejemplo de implementación de una Deep Q-Network (DQN) utilizando un entorno ya predefinido en OpenAI.\n",
    "\n",
    "Tanto para este ejemplo como para las posteriores prácticas se utilizará el <i>framework</i> de __Pytorch__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Entorno CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo usaremos un entorno ya definido en la librería de OpenAI, pero hay que tener presente que en otros problemas más concretos el entorno necesitará ser definido.\n",
    "\n",
    "CartPole consiste en aprender a controlar un objeto. El juego consta de una carretilla y de un palo colocado verticalmente encima de la carretilla. El palo se aguanta únicamente por la gravedad, mientras que la carretilla se mueve a derecha e izquierda sin parar. El objetivo del agente es controlar la velocidad de la carretilla, aumentándola o disminuyéndola con tal de evitar que el palo se caiga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Establecer el entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar cargaremos la librería __gym__ e inicializaremos el entorno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_global = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada entorno tiene definido todo lo necesario para que un agente pueda aprender: tenemos un juego que funciona de una manera determinada y nosotros podemos entrenar a un agente para que aprenda a jugar a ese juego sin ninguna ayuda más que la de experimentar con él observando, actuando y recibiendo recompensas. Así, el entorno del juego ya define qué acciones se pueden tomar, qué situaciones pueden presentarse, en qué consistirá la recompensa, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, podemos visualizar el entorno de __CartPole__ generando un bucle sobre unos pocos episodios y, al terminar, lo cerramos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizamos el entorno\n",
    "for i_episode in range(15):\n",
    "    observation = env_global.reset()\n",
    "    for t in range(100):\n",
    "        env_global.render() #EL RENDER SÓLO FUNCIONA EN LOCAL: comentar línea si no se está en local.\n",
    "        print(observation)\n",
    "        action = env_global.action_space.sample() #acción aleatoria\n",
    "        observation, reward, done, info = env_global.step(action) #ejecución de la acción elegida\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1)) \n",
    "            break\n",
    "env_global.close() #cerramos la visualización del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La recompensa es 1 por cada paso dado, incluyendo el estado terminal. Se considera el entorno resuelto cuando la media de las recompensas es mayor o igual a 195.0 tras 100 intentos consecutivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de una DQN: enseñar a un agente a jugar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La construcción de una DQN para enseñar a un agente a jugar a **CartPole** pasa, como hemos visto en el módulo didáctico, por los siguientes pasos:\n",
    "\n",
    "<ol>\n",
    "    <li> Definir el modelo de red neuronal. </li>\n",
    "    <li> Definir el agente: cómo se debe comportar, cuándo debe seleccionar una acción y cómo. </li>\n",
    "    <li> Fijar hiperparámetros. </li>\n",
    "    <li> Entrenar al agente.  </li>\n",
    "    \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos importando la librería para trabajar en **Pytorch** y otras librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Definición del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es definir nuestra red neuronal, la DQN. Para este ejemplo, usaremos una red neuronal muy sencilla con tres capas lineales y dos capas ReLU, además del optimizador Adam.\n",
    "\n",
    "También indicaremos la posibilidad de trabajar con **CPU** o **CUDA** por si se tiene la opción, puesto que en aprendizaje por refuerzo la mayoría de los procesos suelen requerir mucha máquina y la aceleración por hardware es normalmente necesaria. **Este ejemplo se puede ejecutar con CPU**.\n",
    "\n",
    "\n",
    "Como se explicaba en el módulo teórico, para que el aprendizaje prospere es importante que las aproximaciones de _Q_ sean lo suficientemente buenas para que las experiencias aporten información relevante al agente. Si no se consiguen buenos valores, el agente corre el riesgo de estancarse entre decisiones malas sin mostrar ninguna mejora. Para ello se introduce el **método <i>e-greedy</i>**, que permite al agente explorar acciones aleatorias durante un tiempo al inicio del entrenamiento y facilita que vaya pasando a utilizar la aproximación de _Q_ poco a poco (explotación). Recordemos que este comportamiento viene definido por el hiperparámetro de probabilidad <i>epsilon</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOMW16j-ZFDX"
   },
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cpu'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ### Construcción de la red neuronal\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, self.n_outputs, bias=True))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        \n",
    "        ### Se ofrece la opción de trabajar con CUDA\n",
    "        if self.device == 'cuda':\n",
    "            self.model.cuda()\n",
    "            \n",
    "    \n",
    "    ### Método e-greedy\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acción aleatoria\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acción a partir del cálculo del valor de Q para esa acción\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. _Buffer_ de repetición de experiencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otro problema del algoritmo básico de la DQN era la secuencialidad de los datos: los estados están muy correlacionados y la red neuronal no puede funcionar bien con tanta correlación. Introduciendo un **<i>buffer</i> de repetición de experiencias** permitimos que se almacenen unas cuantas experiencias pasadas y que se pase un subconjunto aleatorio de estas a la red neuronal. A su vez, el *buffer* se tiene que ir alimentando de experiencias nuevas conforme el agente va aprendiendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importamos primero las funciones `deque` y `namedtuple` de la librería `collections`. El *deque* es un objeto que almacena valores hasta un límite fijado. Cuando se llega al límite, el *deque* elimina el primer valor para que pueda entrar uno nuevo, y así sucesívamente, facilitando, en nuestro caso, la retroalimentación del *buffer* con experiencias más nuevas y cada vez más relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos el __<i>buffer</i> de repetición de experiencias__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9F0QBdxTZKJl"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    ##Creamos una lista de índices aleatorios y empaquetamos las experiencias en arrays de Numpy (facilita el cálculo posterior de la pérdida)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    ## Se añaden las nuevas experiencias \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    ## Rellenamos el buffer con experiencias aleatorias al inicio del entrenamiento\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El *burn-in* nos permite rellenar el *buffer* al inicio del entrenamiento (cuando el agente aún no ha empezado a explorar) con experiencias aleatorias para que esté suficientemente lleno como para empezar a entrenar con una variedad de información bastante amplia. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Definición del agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos el modelo definido, solo nos queda definir el comportamiento del agente, el cómo aprende.\n",
    "\n",
    "Recordemos que la última mejora que hacíamos a la DQN básica y que nos permitía establecer el algoritmo DQN final era la introducción de una **red objetivo**. Con esta segunda red (copia exacta de la principal), calculamos el valor objetivo _Q'_ mientras que con la red principal calculamos el valor de _Q_ actual. Y cada cierto tiempo se sincronizan las dos redes. Así, conseguimos evitar que el agente se estanque en una región debido a que la diferencia entre estados sea tan pequeña (correlación), que siempre elija la misma acción y que termine por aprender erróneamente. \n",
    "\n",
    "Básicamente, el proceso que seguirá el agente será el siguiente:\n",
    "<ol>\n",
    "    <li> Rellenar el _buffer_ con unas cuantas experiencias aleatorias. </li>\n",
    "    <li> Interactuar con el entorno (dar un paso): \n",
    "        <ul>\n",
    "            <li> Tomar acción según la probabilidad <i>epsilon</i>. </li>\n",
    "            <li> Almacenar la información en el <i>buffer</i>. </li>\n",
    "            <li> Obtener la recompensa si se encuentra al final del episodio en cuestión. </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li> Actualizar la red neuronal con la frecuencia que se establezca y calcular la pérdida. </li>\n",
    "    <li> Sincronizar la red principal con la red objetivo con la frecuencia que se establezca. </li>\n",
    "    <li> Calcular la media de las recompensas de los últimos _X_ episodios (generalmente 100). </li>\n",
    "    <li> Modificar el valor de <i>epsilon</i> para favorecer la explotación frente a la exploración. </li>\n",
    "</ol>\n",
    "\n",
    "El agente repetirá este proceso hasta que consiga el objetivo a partir del cual se considera que ha aprendido a jugar (en **CartPole** es 195, como se indica en la variable `env.spec.reward_threshold`) o hasta que se agote el límite máximo de episodios establecido (hiperparámetro fijado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPcOyH4GY1mN"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork\n",
    "        self.target_network = deepcopy(dnnetwork) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.nblock = 100 # bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        self.reward_threshold = self.env.spec.reward_threshold # recompensa media a partir de la cual se considera\n",
    "                                                               # que el agente ha aprendido a jugar\n",
    "        self.initialize()\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()\n",
    "        \n",
    "    \n",
    "    ## Tomamos una nueva acción\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore': \n",
    "            # acción aleatoria en el burn-in y en la fase de exploración (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Realizamos la acción y obtenemos el nuevo estado y la recompensa\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardamos experiencia en el buffer\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()\n",
    "        return done\n",
    "\n",
    "    \n",
    "        \n",
    "    ## Entrenamiento\n",
    "    def train(self, gamma=0.99, max_episodes=50000, \n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "               \n",
    "                # Actualizamos la red principal según la frecuencia establecida\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Sincronizamos la red principal y la red objetivo según la frecuencia establecida\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                \n",
    "                if gamedone:                   \n",
    "                    episode += 1\n",
    "                    self.training_rewards.append(self.total_reward) # guardamos las recompensas obtenidas\n",
    "                    self.update_loss = []\n",
    "                    mean_rewards = np.mean(   # calculamos la media de recompensa de los últimos X episodios\n",
    "                        self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "                    \n",
    "                    # Comprobamos que todavía quedan episodios\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    # Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego  \n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "                    \n",
    "                    # Actualizamos epsilon según la velocidad de decaimiento fijada\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "                    \n",
    "                \n",
    "    ## Cálculo de la pérdida                   \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores \n",
    "        states, actions, rewards, dones, next_states = [i for i in batch] \n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.dnnetwork.device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(\n",
    "            device=self.dnnetwork.device)\n",
    "        dones_t = torch.ByteTensor(dones).to(device=self.dnnetwork.device)\n",
    "        \n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenemos los valores de Q objetivo. El parámetro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "        \n",
    "        # Calculamos la ecuación de Bellman\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "        \n",
    "        # Calculamos la pérdida\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    \n",
    "    def update(self):\n",
    "        self.dnnetwork.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.dnnetwork.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())\n",
    "            \n",
    "\n",
    "\n",
    "    def plot_rewards(self):\n",
    "        plt.figure(figsize=(12,8))\n",
    "        plt.plot(self.training_rewards, label='Rewards')\n",
    "        plt.plot(self.mean_training_rewards, label='Mean Rewards')\n",
    "        plt.axhline(self.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "        plt.xlabel('Episodes')\n",
    "        plt.ylabel('Rewards')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fijamos los hiperparámetros necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2625S-lDWxag"
   },
   "outputs": [],
   "source": [
    "lr = 0.001            #Velocidad de aprendizaje\n",
    "MEMORY_SIZE = 100000  #Máxima capacidad del buffer\n",
    "MAX_EPISODES = 5000   #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .99   #Decaimiento de epsilon\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "BATCH_SIZE = 32       #Conjunto a coger del buffer para la red neuronal\n",
    "BURN_IN = 1000        #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n",
    "DNN_UPD = 1           #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 2500       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el *buffer* de repetición de experiencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el modelo de red neuronal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(env_global, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos nuestro agente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env_global, dqn, buffer, EPSILON, EPSILON_DECAY, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos al agente con los hiperparámetros establecidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agusIfEMZlKi",
    "outputId": "47b8253f-5857-4b63-d60c-442b983d1d3c"
   },
   "outputs": [],
   "source": [
    "agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, \n",
    "              batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Representar el aprendizaje del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "9ghHg3rH3MTV",
    "outputId": "a7c5d4cb-5e98-45ac-ca70-8dcfdffaa86a"
   },
   "outputs": [],
   "source": [
    "agent.plot_rewards()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "OriginalBlog_mod.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "M2983PRAX007",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a6adb791819b312cb85d3f7f95bade2111798fa581516345d5c0e47cc9c9fdca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
